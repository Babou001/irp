# ğŸ“¦ Models Required

**IMPORTANT**: Les modÃ¨les ML ne sont PAS inclus dans le repository Git car ils sont trop volumineux (>2 GB). Vous devez les tÃ©lÃ©charger sÃ©parÃ©ment.

## ğŸ“¥ ModÃ¨les Ã  TÃ©lÃ©charger

### 1. ModÃ¨le d'Embeddings (all-mpnet-base-v2)

**Taille**: ~420 MB

**MÃ©thode 1 - Via Hugging Face Hub** (RecommandÃ©):
```bash
pip install huggingface-hub

# TÃ©lÃ©charger le modÃ¨le
python -c "from huggingface_hub import snapshot_download; snapshot_download(repo_id='sentence-transformers/all-mpnet-base-v2', local_dir='./models/all-mpnet-base-v2')"
```

**MÃ©thode 2 - Manuellement**:
1. Aller sur https://huggingface.co/sentence-transformers/all-mpnet-base-v2
2. TÃ©lÃ©charger tous les fichiers dans `models/all-mpnet-base-v2/`

**Structure attendue**:
```
models/all-mpnet-base-v2/
â”œâ”€â”€ config.json
â”œâ”€â”€ pytorch_model.bin
â”œâ”€â”€ tokenizer_config.json
â”œâ”€â”€ vocab.txt
â”œâ”€â”€ special_tokens_map.json
â””â”€â”€ ... (autres fichiers)
```

---

### 2. ModÃ¨le LLM Llama 3.2 3B (QuantifiÃ© Q5)

**Taille**: ~2.3 GB

**MÃ©thode 1 - Via Hugging Face** (RecommandÃ©):
```bash
# Installer huggingface-cli
pip install huggingface-hub

# TÃ©lÃ©charger le modÃ¨le
huggingface-cli download bartowski/Llama-3.2-3B-Instruct-GGUF \
    Llama-3.2-3B-Instruct-Q5_K_L.gguf \
    --local-dir ./models \
    --local-dir-use-symlinks False
```

**MÃ©thode 2 - Manuellement**:
1. Aller sur https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF
2. Cliquer sur "Files and versions"
3. TÃ©lÃ©charger `Llama-3.2-3B-Instruct-Q5_K_L.gguf`
4. Le placer dans `models/Llama-3.2-3B-Instruct-Q5_K_L.gguf`

**Structure attendue**:
```
models/
â””â”€â”€ Llama-3.2-3B-Instruct-Q5_K_L.gguf  (2.3 GB)
```

---

## âœ… VÃ©rification

AprÃ¨s tÃ©lÃ©chargement, vÃ©rifiez que vous avez cette structure :

```
version_using_milvus/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ all-mpnet-base-v2/
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”‚   â””â”€â”€ ... (autres fichiers)
â”‚   â””â”€â”€ Llama-3.2-3B-Instruct-Q5_K_L.gguf
â”œâ”€â”€ data/
â”œâ”€â”€ fast_api_app.py
â””â”€â”€ ... (autres fichiers)
```

VÃ©rifier la taille des fichiers :
```bash
ls -lh models/all-mpnet-base-v2/
ls -lh models/Llama-3.2-3B-Instruct-Q5_K_L.gguf
```

---

## ğŸš€ AprÃ¨s le TÃ©lÃ©chargement

Une fois les modÃ¨les tÃ©lÃ©chargÃ©s, vous pouvez :

### Mode DÃ©veloppement (avec volumes Docker) :
```bash
docker-compose up --build -d
```

### Mode Production (modÃ¨les embarquÃ©s dans l'image) :
```bash
./scripts/build-production-image.sh --export
docker-compose -f docker-compose.prod.yml up -d
```

---

## âš ï¸ Notes Importantes

1. **Espace disque requis** :
   - ModÃ¨les : ~2.7 GB
   - Build Docker production : ~20-25 GB temporaires
   - Image finale : ~5-7 GB

2. **ConfidentialitÃ©** :
   - Les modÃ¨les s'exÃ©cutent 100% en local
   - Aucune donnÃ©e n'est envoyÃ©e vers l'extÃ©rieur
   - Parfait pour environnements air-gapped

3. **Alternatives** :
   - Si `Llama-3.2-3B-Instruct-Q5_K_L.gguf` n'est pas disponible
   - Vous pouvez utiliser `Q4_K_L` (plus petit, moins performant)
   - Ou `Q6_K` (plus gros, plus performant)
   - Modifier `paths.py` ligne 17 si nÃ©cessaire

---

## ğŸ“š Documentation

- Guide de dÃ©ploiement complet : [README_DEPLOYMENT.md](README_DEPLOYMENT.md)
- DÃ©ploiement air-gapped : [docs/AIRGAPPED.md](docs/AIRGAPPED.md)
- DÃ©marrage rapide : [docs/QUICKSTART.md](docs/QUICKSTART.md)
